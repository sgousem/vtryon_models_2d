{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9d567ef9cdcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# change the given value of 2.15 according to the size of the detected face\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mglasses_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.16\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0moverlay_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglass_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "# read both the images of the face and the glasses\n",
    "#image = cv2.imread('sample.jpg')\n",
    "glass_img = cv2.imread('glass_image.jpg')\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, image = cap.read() \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    centers = []\n",
    "    faces = face_cascade.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "    # iterating over the face detected\n",
    "    for (x, y, w, h) in faces:\n",
    "\n",
    "        # create two Regions of Interest.\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = image[y:y + h, x:x + w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        # Store the coordinates of eyes in the image to the 'center' array\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            centers.append((x + int(ex + 0.5 * ew), y + int(ey + 0.5 * eh)))\n",
    "\n",
    "    if len(centers) > 0:\n",
    "        # change the given value of 2.15 according to the size of the detected face\n",
    "        glasses_width = 2.16 * abs(centers[1][0] - centers[0][0])\n",
    "        overlay_img = np.ones(image.shape, np.uint8) * 255\n",
    "        h, w = glass_img.shape[:2]\n",
    "        scaling_factor = glasses_width / w\n",
    "\n",
    "        overlay_glasses = cv2.resize(glass_img, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        x = centers[0][0] if centers[0][0] < centers[1][0] else centers[1][0]\n",
    "\n",
    "        # The x and y variables below depend upon the size of the detected face.\n",
    "        x -= 0.26 * overlay_glasses.shape[1]\n",
    "        y += 0.85 * overlay_glasses.shape[0]\n",
    "\n",
    "        # Slice the height, width of the overlay image.\n",
    "        h, w = overlay_glasses.shape[:2]\n",
    "        overlay_img[int(y):int(y + h), int(x):int(x + w)] = overlay_glasses\n",
    "\n",
    "        # Create a mask and generate it's inverse.\n",
    "        gray_glasses = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)\n",
    "        ret, mask = cv2.threshold(gray_glasses, 110, 255, cv2.THRESH_BINARY)\n",
    "        mask_inv = cv2.bitwise_not(mask)\n",
    "        temp = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "        temp2 = cv2.bitwise_and(overlay_img, overlay_img, mask=mask_inv)\n",
    "        final_img = cv2.add(temp, temp2)\n",
    "\n",
    "        # imS = cv2.resize(final_img, (1366, 768))\n",
    "        cv2.imshow('Lets wear Glasses', final_img)\n",
    "        k = cv2.waitKey(0) & 0xFF\n",
    "        if k == ord('q'): # wait for ESC key to exit\n",
    "            break\n",
    "        #if cv2.waitKey(0) & 0xFF == ord('s'):\n",
    "            #break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
